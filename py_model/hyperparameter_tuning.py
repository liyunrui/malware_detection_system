import warnings
warnings.filterwarnings("ignore")
from glob import glob
import gc
import os
from tqdm import tqdm
import time
from itertools import chain
import pandas as pd
import numpy as np
from xgboost import plot_importance
from xgboost import XGBClassifier
import utils # made by author for efficiently dealing with data
from sklearn.grid_search import GridSearchCV
from datetime import datetime

seed = 72
np.random.seed(seed)

train = pd.read_csv('../feature/{}/all_features.csv.gz'.format('train'), compression='gzip')
test = pd.read_csv('../feature/{}/all_features.csv.gz'.format('test'), compression='gzip')

train['final_time'] = pd.to_datetime(train.final_time)
train['initial_time'] = pd.to_datetime(train.initial_time)

print( train.shape)
print( test.shape)

print( 'scale_pos_weight',  1.0 * train.label.value_counts().iloc[0] / train.label.value_counts().iloc[1])
ratio = 1.0 * train.label.value_counts().iloc[0] / train.label.value_counts().iloc[1]
train.head()
#==============================================================================
# prepare training data
#==============================================================================
Y_train = train['label'] 
X_train = train.drop(['label'], axis=1)
#X_train.drop(['id','initial_time','final_time'], axis=1, inplace = True)
del train
print ('prepartion of training set is done')

col = ['duration','min-event_min_of_the_hour','diff2_user_ratio-interval_3_freq_1440min',
    'log_last_user_ratio_over_time-interval_3_freq_1440min','ratio_product_over_num_user']
#col = ['duration','min-event_week_of_month']
X_train.drop(col, axis = 1, inplace = True)
X_train.shape


def train_val_split(valid_size = 0.2):
    #--------------------------
    # train/val split by time period
    #--------------------------
    # randomly pick computer program as validation user from 2017-04-03
    splitting_time = datetime.strptime('2017-04-03', '%Y-%m-%d')    
    valid_id = X_train[X_train['initial_time'] > splitting_time].sample(n = int(X_train.shape[0] * valid_size), random_state = seed ).id 
    is_valid = X_train.id.isin(valid_id) # return a boolean sereis
    #-----
    # train
    #-----
    x_train = X_train[~is_valid].drop(['id','initial_time','final_time'], axis=1)
    y_train = Y_train[~is_valid]
    # for scale_pos_weight
    ratio = 1.0 * y_train.value_counts().iloc[0] / y_train.value_counts().iloc[1]
    #-----
    # val
    #-----
    x_val = X_train[is_valid].drop(['id','initial_time','final_time'], axis=1)
    y_val = Y_train[is_valid]
    
    
    print('FINAL SHAPE')
    print('x_train.shape:{0}'.format(x_train.shape))
    print('x_val.shape:{0}'.format(x_val.shape))
    print('Splitting is fine.') if X_train.shape[0] == x_train.shape[0] + x_val.shape[0] else print('oops')
    return x_train, y_train, x_val, y_val, ratio

#==============================================================================
print('training')
#==============================================================================
# xgboost parameters (tuning result from cv)
params_fixed = {
    'objective' : 'binary:logistic',
    'max_depth': 17,
    'min_child_weight': 1,
    'gamma': 0.0,
    'subsample' : 0.9,
    'colsample_bytree': 0.6,
}



# hold-out validation
x_train, y_train, x_val, y_val, ratio = train_val_split(valid_size = 0.2)
# model training
s = time.time()
model = XGBClassifier(
    **params_fixed,
    scale_pos_weight = ratio,
                       n_estimators = 1350,
                       learning_rate = 0.01,
                       seed = seed 
                         )
model.fit(x_train, y_train, 
      eval_metric ='auc' ,eval_set = [(x_val, y_val)],
      early_stopping_rounds = 50) 
e = time.time()
print ('sec : ', e - s)