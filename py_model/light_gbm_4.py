#!/usr/bin/python3
# -*-coding:utf-8
'''
Created on Sun Mar 18 2018

@author: Ray

'''

import warnings
warnings.filterwarnings("ignore")
import pandas as pd
from xgboost import plot_importance
from lightgbm import LGBMClassifier
import time
from datetime import datetime
from xgboost import plot_importance
import numpy as np
import pickle # for saving
import utils # made by author for efficiently dealing with data
import gc


###############################
# setting
###############################

# using different random seed to make sure variety of models
seed = 74
np.random.seed(seed) 

DATE = '0318'
utils.mkdir_p('../output/model/{}_{}/'.format(DATE,seed))
utils.mkdir_p('../output/sub/{}_{}/'.format(DATE,seed))

print("""#==== print param ======""")
print('DATE:', DATE)
print('seed:', seed)

##################################
# loading data
##################################
train = utils.load_pred_feature_for_lightgbm('train', keep_all = False)

print( 'scale_pos_weight',  1.0 * train.label.value_counts().iloc[0] / train.label.value_counts().iloc[1])

##################################
# pre-processing
##################################
train['final_time'] = pd.to_datetime(train.final_time)
train['initial_time'] = pd.to_datetime(train.initial_time)
print('pre-processing done')

#==============================================================================
# prepare training data
#==============================================================================
Y_train = train['label'] 
X_train = train.drop(['label'], axis=1)

print ('prepartion of training set is done')


def train_val_split(valid_size = 0.2):
    #--------------------------
    # train/val split by time period
    #--------------------------
    # randomly pick computer program as validation user from 2017-04-03
    splitting_time = datetime.strptime('2017-04-03', '%Y-%m-%d')    
    valid_id = X_train[X_train['initial_time'] > splitting_time].sample(n = int(X_train.shape[0] * valid_size), random_state = seed ).id 
    is_valid = X_train.id.isin(valid_id) # return a boolean sereis
    #-----
    # train
    #-----
    x_train = X_train[~is_valid].drop(['id','initial_time','final_time'], axis=1)
    y_train = Y_train[~is_valid]
    # for scale_pos_weight
    ratio = 1.0 * y_train.value_counts().iloc[0] / y_train.value_counts().iloc[1]
    #-----
    # val
    #-----
    x_val = X_train[is_valid].drop(['id','initial_time','final_time'], axis=1)
    y_val = Y_train[is_valid]
    
    
    print('FINAL SHAPE')
    print('x_train.shape:{0}'.format(x_train.shape))
    print('x_val.shape:{0}'.format(x_val.shape))
    print('Splitting is fine.') if X_train.shape[0] == x_train.shape[0] + x_val.shape[0] else print('oops')
    return x_train, y_train, x_val, y_val, ratio


#==============================================================================
print('training')
#==============================================================================
#Here we did is lower the learning rate and add more trees.
n_estimators = 10000
learning_rate = 0.01
early_stopping_rounds = 50
# for simple ensemble
LOOP = 2
# Core
models = [] # for the following prediction
for i in range(LOOP):
    print('LOOP',i)
    # hold-out validation
    x_train, y_train, x_val, y_val, ratio = train_val_split(valid_size = 0.2)
    # model
    model = LGBMClassifier(objective = 'binary',
                           scale_pos_weight = ratio,
                           seed = seed,
                           n_estimators = n_estimators,
                           learning_rate = learning_rate                                                      
                          )
    model.fit(x_train, y_train, 
          eval_metric ='auc' ,eval_set = [(x_val, y_val)],
          early_stopping_rounds = 50) 
    models.append(model)
    # saving( model 一定要養成習慣保存, 因為訓練需要很大時間, 萬一有意外...)
    models.append(model)
    pickle.dump(model, open('../output/model/{}_{}/lightgbm_{}.model'.format(DATE, seed, i), "wb"))    
    # validating
    valid_yhat = model.predict_proba(x_val)[:,1] # y_hat is result of prediction
    print('Valid Mean:', np.mean(valid_yhat))
    del x_train, y_train, x_val, y_val
    gc.collect()

#==============================================================================
print('test')
#==============================================================================
#load testing set
test = utils.load_pred_feature_for_lightgbm('test', keep_all = False)

sub_test = test[['id']]
sub_test.columns = ['FileID'] 
test.drop(['id','label','initial_time','final_time'], axis = 1, inplace = True) # remove sth which is not features

#Core
sub_test['Probability'] = 0 

for model in models:
    sub_test['Probability'] += model.predict_proba(test)[:,1]
sub_test['Probability'] /= LOOP # do some simple ensemble: average of prediting result

print('Test Mean:', sub_test['Probability'].mean())

'''
Here we check the valid and test prediction distribution.

'''
# saving for submitting
sub_test.to_csv('../output/sub/{}_{}/sub_test_lightgbm.csv'.format(DATE,seed), index = False)




