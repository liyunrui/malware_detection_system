import pandas as pd
import numpy as np
import datetime # datetime preprocessing
import utils # made by author for efficiently dealing with data
import gc
##################################
# loading data
##################################
train = pd.read_csv('../input/train.csv.gz', compression='gzip', dtype={'ProductID': str})
test = pd.read_csv('../input/test.csv.gz', compression='gzip', dtype={'ProductID': str})

print('loading finish')

##################################
# pre-processing
##################################

# convert query_datetime(str type) column to datetime type
train['query_datetime'] = pd.to_datetime(train.query_datetime)
test['query_datetime'] = pd.to_datetime(test.query_datetime)

# ProductID Cleaning
train['ProductID'] = train.ProductID.map(str)
train['ProductID'] = train.ProductID.apply(lambda x: '055649' if x == '55649' else x)
test['ProductID'] = test.ProductID.map(str)
test['ProductID'] = test.ProductID.apply(lambda x: '055649' if x == '55649' else x)

print('pre-processing done')


##################################
#rolling statistics calculation(make temporal context embedded in the feature)
##################################
#這個程式開始的時間點之前window內, 發生幾次可疑檔案紀錄(用來把當下時間點的temporal context embed into feature)
    # window = 30sec, 前30秒內
    # window = 1min, 前一分鐘內
    # window = 30min, 前30分鐘內
    # window = 60min, 前一小時內
    # window = 120min, 前兩小時內
    # window = 1day, 前一天內
#這個程式進行到一半的時間點之前window內, 發生幾次可疑檔案紀錄(用來把當下時間點的temporal context embed into feature)
    # window = 60min, 前一小時內
    # window = 120min, 前兩小時內
    # window = 1day, 前一天內
    # window = 30 min,前30分鐘內
    # window = 1 min, 前60秒內
#這個程式結束的時間點之前window內, 發生幾次可疑檔案紀錄(用來把當下時間點的temporal context embed into feature)
    # window = 60min, 前一小時內
    # window = 120min, 前兩小時內
    # window = 1day, 前一天內
    # window = 30 min,前30分鐘內
    # window = 1 min, 前60秒內    

# using test + train
df_all = pd.concat((train, test))[['id', 'query_datetime']]
df_counts = df_all.set_index('query_datetime')[['id']].sort_index() 
#在這個時間點, 發生多少次可以的紀錄
df_counts = df_counts.groupby('query_datetime').count().rename(columns = {'id': 'num_suspicious_records'})

del df_all
gc.collect()
#------------
# rolling window ---> .sum(), .min(), .max(), .std(), .mean()
#------------

rolling_time = ['30s', '1min', '30min','60min','120min','1440min']
for window in rolling_time:    
    #creat temporal feature (這個時間點之前, 總共發生幾次可以的紀錄)
    df_counts['sum-count_rolling_{}'.format(window)] =  df_counts[['num_suspicious_records']].rolling(window).sum()
    #creat temporal feature (這個時間點之前, 平均發生幾次可以的紀錄)
    df_counts['mean-count_rolling_{}'.format(window)] =  df_counts[['num_suspicious_records']].rolling(window).mean()
    #creat temporal feature (這個時間點之前, 發生可疑紀錄的變動程度)
    df_counts['std-count_rolling_{}'.format(window)] =  df_counts[['num_suspicious_records']].rolling(window).std()
    #creat temporal feature (這個時間點之前, 最多發生幾次可疑紀錄)
    df_counts['max-count_rolling_{}'.format(window)] =  df_counts[['num_suspicious_records']].rolling(window).max()
    #creat temporal feature (這個時間點之前, 最少發生幾次可疑紀錄)
    df_counts['min-count_rolling_{}'.format(window)] =  df_counts[['num_suspicious_records']].rolling(window).min()

'''
Note that:
    std- will create NaN at time in the beginning, but xgboost can handle null value.
'''
df_counts.reset_index('query_datetime', inplace = True) # for subsequent merge

#-------------------------
#train
#-------------------------

#把每一個程式開始的時間當作query time
df1 = train.groupby('id').query_datetime.min().to_frame('query_datetime').reset_index()
df1 = pd.merge(df1, df_counts, on = 'query_datetime', how = 'left')
df1.drop('query_datetime', axis = 1, inplace = True)
df1 = df1.add_prefix('initial-')
df1.rename(columns={'initial-id': 'id'}, inplace = True)
#把每一個程式結束的時間當作query time
df2 = train.groupby('id').query_datetime.max().to_frame('query_datetime').reset_index()
df2 = pd.merge(df2, df_counts, on = 'query_datetime', how = 'left')
df2.drop('query_datetime', axis = 1, inplace = True)
df2 = df2.add_prefix('final-')
df2.rename(columns={'final-id': 'id'}, inplace = True)
#把每一個程式進行到中間的時間當作query time
df3 = train.groupby('id').query_datetime.apply(lambda x: x.iloc[int(len(x.tolist())/2)]).to_frame('query_datetime').reset_index()
df3 = pd.merge(df3, df_counts, on = 'query_datetime', how = 'left')
df3.drop('query_datetime', axis = 1, inplace = True)
df3 = df3.add_prefix('middle-')
df3.rename(columns={'middle-id': 'id'}, inplace = True)

train = pd.merge(df1,df2, on = 'id', how = 'left')
train = pd.merge(train,df3, on = 'id', how = 'left')
del df1, df2, df3

#-------------------------
#test
#-------------------------

#把每一個程式開始的時間當作query time
df1 = test.groupby('id').query_datetime.min().to_frame('query_datetime').reset_index()
df1 = pd.merge(df1, df_counts, on = 'query_datetime', how = 'left')
df1.drop('query_datetime', axis = 1, inplace = True)
df1 = df1.add_prefix('initial-')
df1.rename(columns={'initial-id': 'id'}, inplace = True)
#把每一個程式結束的時間當作query time
df2 = test.groupby('id').query_datetime.max().to_frame('query_datetime').reset_index()
df2 = pd.merge(df2, df_counts, on = 'query_datetime', how = 'left')
df2.drop('query_datetime', axis = 1, inplace = True)
df2 = df2.add_prefix('final-')
df2.rename(columns={'final-id': 'id'}, inplace = True)
#把每一個程式進行到中間的時間當作query time
df3 = test.groupby('id').query_datetime.apply(lambda x: x.iloc[int(len(x.tolist())/2)]).to_frame('query_datetime').reset_index()
df3 = pd.merge(df3, df_counts, on = 'query_datetime', how = 'left')
df3.drop('query_datetime', axis = 1, inplace = True)
df3 = df3.add_prefix('middle-')
df3.rename(columns={'middle-id': 'id'}, inplace = True)

test = pd.merge(df1,df2, on = 'id', how = 'left')
test = pd.merge(test,df3, on = 'id', how = 'left')
del df1, df2, df3
del df_counts
#-------------------------
#save
#-------------------------

train.to_csv('../feature/{}/temporal_context.csv.gz'.format('train'), index = False, compression='gzip')
test.to_csv('../feature/{}/temporal_context.csv.gz'.format('test'), index = False, compression='gzip')
