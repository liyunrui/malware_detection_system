import pandas as pd
import numpy as np
import datetime # datetime preprocessing
import utils # made by author for efficiently dealing with data

##################################
# loading data
##################################
train = pd.read_csv('../input/train.csv.gz', compression='gzip', dtype={'ProductID': str})
test = pd.read_csv('../input/test.csv.gz', compression='gzip', dtype={'ProductID': str})

print('loading finish')

##################################
# pre-processing
##################################

# convert query_datetime(str type) column to datetime type
train['query_datetime'] = pd.to_datetime(train.query_datetime)
test['query_datetime'] = pd.to_datetime(test.query_datetime)


def psersonal_estimation_personal_freq(x):
    '''
    std:代表的是數值的分散程度
    
    parameters:
    --------
    x: DataFrame
    '''
    x['std-personal_freq'] = x.personal_freq.std()
    x['mean-personal_freq'] = x.personal_freq.mean()
    x['min-personal_freq'] = x.personal_freq.min()
    x['max-personal_freq'] = x.personal_freq.max()
    #remove useful columns
    x.drop(['first_time','last_time','personal_count','psersonal_duration','personal_freq'], axis = 1, inplace = True)
    return x

#-------------------------
# train
#-------------------------
#計算每個消費者個別使用此程式的持續時間
df1 = train.groupby(by = ['id','CustomerID']).query_datetime.min().to_frame('first_time')
df2 = train.groupby(by = ['id','CustomerID']).query_datetime.max().to_frame('last_time')
df1 = pd.concat([df1,df2], axis =1)
del df2
df1['psersonal_duration'] = (df1['last_time'] - df1['first_time']).map(lambda x: x.days)
df1 = df1.reset_index('id').reset_index('CustomerID')
#計算每個消費者個別使用此程式的次數
df2 =train.groupby(by = ['id','CustomerID']).count()[['label']] \
.rename(columns = {'label': 'personal_count'}) \
.reset_index('CustomerID') \
.reset_index('id')
#merge
train = pd.merge(df1,df2, on = ['id','CustomerID'], how = 'left')
del df1, df2
#計算每個消費者個別使用此程式的freq
train['personal_freq'] = train['personal_count'] / (train.psersonal_duration + 1) # smoothing
#output
train = train.groupby('id').apply(psersonal_estimation_personal_freq).reset_index() \
.drop_duplicates('id')[['id','std-personal_freq','mean-personal_freq',
                       'min-personal_freq','max-personal_freq']]
#-------------------------
# test
#-------------------------
#計算每個消費者個別使用此程式的持續時間
df1 = test.groupby(by = ['id','CustomerID']).query_datetime.min().to_frame('first_time')
df2 = test.groupby(by = ['id','CustomerID']).query_datetime.max().to_frame('last_time')
df1 = pd.concat([df1,df2], axis =1)
del df2
df1['psersonal_duration'] = (df1['last_time'] - df1['first_time']).map(lambda x: x.days)
df1 = df1.reset_index('id').reset_index('CustomerID')
#計算每個消費者個別使用此程式的次數
df2 =test.groupby(by = ['id','CustomerID']).count()[['label']] \
.rename(columns = {'label': 'personal_count'}) \
.reset_index('CustomerID') \
.reset_index('id')
#merge
test = pd.merge(df1,df2, on = ['id','CustomerID'], how = 'left')
del df1, df2
#計算每個消費者個別使用此程式的freq
test['personal_freq'] = test['personal_count'] / (test.psersonal_duration + 1) # smoothing
#output
test = test.groupby('id').apply(psersonal_estimation_personal_freq).reset_index() \
.drop_duplicates('id')[['id','std-personal_freq','mean-personal_freq',
                       'min-personal_freq','max-personal_freq']]

#-------------------------
#save
#-------------------------

train.to_csv('../feature/{}/individual_freq.csv.gz'.format('train'), index = False, compression='gzip')
test.to_csv('../feature/{}/individual_freq.csv.gz'.format('test'), index = False, compression='gzip')
