#!/usr/bin/python3
# -*-coding:utf-8
import time
import pandas as pd
import numpy as np
from tqdm import tqdm
import os
import utils # written by author
import multiprocessing as mp
import gc # for automatic releasing memory

def datetime_feature(df, name):
	#duration
    df = pd.merge(df, pd.read_csv('../feature/{}/duration.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #program_relative_time
    df = pd.merge(df, pd.read_csv('../feature/{}/program_relative_time.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #global_window_3_local_pattern_1440min
    df = pd.merge(df, pd.read_csv('../feature/{}/global_window_3_local_pattern_1440min.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #global_window_6_local_pattern_720min
    df = pd.merge(df, pd.read_csv('../feature/{}/global_window_6_local_pattern_720min.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #global_window_12_local_pattern_360min
    df = pd.merge(df, pd.read_csv('../feature/{}/global_window_12_local_pattern_360min.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #global_window_24_local_pattern_180min
    df = pd.merge(df, pd.read_csv('../feature/{}/global_window_24_local_pattern_180min.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')

    # #global_window_3_local_pattern_1440min_ratio(performance seems worse)----->
    # df = pd.merge(df, pd.read_csv('../feature/{}/global_window_3_local_pattern_1440min_ratio.csv.gz'.format(name), compression='gzip'),
    #               on='id', how='left')
    
    #overlap_time
    df = pd.merge(df, pd.read_csv('../feature/{}/overlap_time.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #day_of_the_week_one_hot_encoding
    df = pd.merge(df, pd.read_csv('../feature/{}/day_of_the_week_one_hot_encoding.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #week_of_month_one_hot_encoding 
    df = pd.merge(df, pd.read_csv('../feature/{}/week_of_month_one_hot_encoding.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')

    gc.collect()
    return df

def customer_feature(df, name):
	#num_user
    df = pd.merge(df, pd.read_csv('../feature/{}/num_user.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #triggering_at_the_same_time
    df = pd.merge(df, pd.read_csv('../feature/{}/triggering_at_the_same_time.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #individual_duration_stats
    df = pd.merge(df, pd.read_csv('../feature/{}/individual_duration_stats.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')

    # #global_window_3_local_pattern_1440min_user (performance seems worse)---->
    # df = pd.merge(df, pd.read_csv('../feature/{}/global_window_3_local_pattern_1440min_user.csv.gz'.format(name), compression='gzip'),
    #               on='id', how='left')
    
    #global_window_3_local_pattern_1440min_user_ratio_over_time
    df = pd.merge(df, pd.read_csv('../feature/{}/global_window_3_local_pattern_1440min_user_ratio_over_time.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #individual_count
    df = pd.merge(df, pd.read_csv('../feature/{}/individual_count.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #individual_freq
    df = pd.merge(df, pd.read_csv('../feature/{}/individual_freq.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #individual_duration_in_sec
    df = pd.merge(df, pd.read_csv('../feature/{}/individual_duration_in_sec.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #num_overlap
    df = pd.merge(df, pd.read_csv('../feature/{}/num_overlap.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #customer_segment_by_user    
    df = pd.merge(df, pd.read_csv('../feature/{}/customer_segment_by_user.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #id_x_user
    df = pd.merge(df, pd.read_csv('../feature/{}/id_x_user.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')

    gc.collect()
    return df
def product_feature(df, name):
    #num_prodcut
    df = pd.merge(df, pd.read_csv('../feature/{}/num_prodcut.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #product_one_hot
    df = pd.merge(df, pd.read_csv('../feature/{}/product_one_hot.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')
    #global_window_3_local_pattern_1440min_product_count
    df = pd.merge(df, pd.read_csv('../feature/{}/global_window_3_local_pattern_1440min_product_count.csv.gz'.format(name), compression='gzip'),
                  on='id', how='left')


    gc.collect()
    return df
def concat_pred_features(T):
    if T == -1:
        name = 'test'
        train = pd.read_csv('../Raw_Data/testing-set.csv', header = None) # 此train代表的是test的user
    else:
        name = 'train'
        #==============================================================================
        print('load label')
        #==============================================================================        
        train = pd.read_csv('../Raw_Data/training-set.csv', header = None)
    
    train.columns = ['id','label']
    #==============================================================================
    print('datetime feature')
    #==============================================================================
    df = datetime_feature(train, name)
    
    print('{}.shape: {}'.format(name, df.shape))

    #==============================================================================
    print('customer feature')
    #==============================================================================
    df = customer_feature(df, name)
    
    print('{}.shape: {}'.format(name, df.shape))

    #==============================================================================
    print('product feature')
    #==============================================================================
    df = product_feature(df, name)
    
    print('{}.shape: {}'.format(name, df.shape))

    #==============================================================================
    print('feature engineering -----> meta feature and higer order feature')
    #==============================================================================

    # 這個程式, 在單位時間內觸及到多少人 (meta feature)
    df['ratio_user_over_time'] = 1.0 * df['count_customer'] / (df['duration'] + 1) # smoothing
    # 這個程式, 單位人數使用多少樣商品
    df['ratio_product_over_num_user'] = 1.0 * df['count_product'] / df['count_customer']    
    # higher order feature
    df['dominance'] = df['ratio_user_over_time'] * df['freq_over_time']
    # 這個程式, 重複時間佔持續時間得比例
    df['ratio_overlap_over_duration'] = df['overlap_time_in_sec'] / df['duration_in_sec']
    # 使用此程式的消費者,平均花多少時間在不同裝置的比例
    df['ratio_id_x_user_over'] = df['mean-num_device_user_used'] / (df['mean-spent_time_on_programe_in_sec'] + 1) # smoothing
    # aggregate two features
        # 1.在這樣的程式持續時間下(duration),平均會涉及多少裝置(count_product),平均會有多少可疑紀錄(total_count), 平均會有多少
        #
    for given_col in ['duration','triggering_at_the_same_time']:
        #  using aggregation to create features
        gby = df.groupby(given_col).mean()[['count_product','total_count','ratio_user_over_time','freq_over_time','count_customer']]
        # feature names
        gby.columns = ['{}_given_{}'.format(col, given_col) for col in gby.columns]
        # merge them with train set
        df = pd.merge(df, gby, how='left', left_on = given_col, right_index=True)
       
    # some features with largest, we perform log transformation to them
    for col in df.columns:
        if 'given' in col and df[col].max() > 100:
            df['log_{}'.format(col)] = np.log(df[col] + 1) # smoothing
            df.drop(col, axis = 1, inplace = True)
        elif df[col].dtypes == np.int64 and df[col].max() > 100:
            print ('col',col)
            df['log_{}'.format(col)] = np.log(df[col] + 1) # smoothing
            df.drop(col, axis = 1, inplace = True)
        elif df[col].dtypes == np.float64 and df[col].max() > 100:
            print ('col',col)
            df['log_{}'.format(col)] = np.log(df[col] + 1) # smoothing
            df.drop(col, axis = 1, inplace = True)
         
    #delta
    df['diff1-interval_3_freq_1440min'] = df['log_middle_period_count-interval_3_freq_1440min'] - df['log_prior_period_count-interval_3_freq_1440min']
    df['diff2-interval_3_freq_1440min'] = df['log_last_period_count-interval_3_freq_1440min'] - df['log_middle_period_count-interval_3_freq_1440min']
    df['diff3-interval_3_freq_1440min'] = df['log_last_period_count-interval_3_freq_1440min'] - df['log_prior_period_count-interval_3_freq_1440min']
    
    df['diff1-interval_6_freq_720min'] = df['log_second_period_state-interval_6_freq_720min'] - df['log_first_period_state-interval_6_freq_720min']
    df['diff2-interval_6_freq_720min'] = df['log_third_period_state-interval_6_freq_720min'] - df['log_second_period_state-interval_6_freq_720min']
    df['diff3-interval_6_freq_720min'] = df['log_fourth_period_state-interval_6_freq_720min'] - df['log_third_period_state-interval_6_freq_720min']
    df['diff4-interval_6_freq_720min'] = df['log_fifth_period_state-interval_6_freq_720min'] - df['log_fourth_period_state-interval_6_freq_720min']
    df['diff5-interval_6_freq_720min'] = df['log_sixth_period_state-interval_6_freq_720min'] - df['log_fifth_period_state-interval_6_freq_720min']
    df['diff6-interval_6_freq_720min'] = df['log_sixth_period_state-interval_6_freq_720min'] - df['log_first_period_state-interval_6_freq_720min']
   
    df['diff1-interval_24_freq_180min'] = df['log_twenty_fourth_period_state-interval_24_freq_180min'] - df['log_first_period_state-interval_24_freq_180min']

    df['diff1_user_ratio-interval_3_freq_1440min'] = df['log_middle_user_ratio_over_time-interval_3_freq_1440min'] - df['log_prior_user_ratio_over_time-interval_3_freq_1440min']
    df['diff2_user_ratio-interval_3_freq_1440min'] = df['log_last_user_ratio_over_time-interval_3_freq_1440min'] - df['log_middle_user_ratio_over_time-interval_3_freq_1440min']
    df['diff3_user_ratio-interval_3_freq_1440min'] = df['log_last_user_ratio_over_time-interval_3_freq_1440min'] - df['log_prior_user_ratio_over_time-interval_3_freq_1440min']


    print('{}.shape: {}'.format(name, df.shape))    
    #==============================================================================
    print('reduce memory')
    #==============================================================================
    utils.reduce_memory(df)
    #==============================================================================
    print('output')
    #==============================================================================
    df.to_csv('../feature/{}/all_features.csv.gz'.format(name), index = False, compression='gzip')

def multi(name):
    concat_pred_features(name)

##################################################
# Main
##################################################
s = time.time()

mp_pool = mp.Pool(2)
mp_pool.map(multi, [1, -1])

e = time.time()
print (e-s)
