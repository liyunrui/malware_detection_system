import pandas as pd
import numpy as np
import datetime # datetime preprocessing
import utils # made by author for efficiently dealing with data
from math import ceil 

##################################
# loading data
##################################
train = pd.read_csv('../input/train.csv.gz', compression='gzip', dtype={'ProductID': str})
test = pd.read_csv('../input/test.csv.gz', compression='gzip', dtype={'ProductID': str})

print('loading finish')

##################################
# pre-processing
##################################

# convert query_datetime(str type) column to datetime type
train['query_datetime'] = pd.to_datetime(train.query_datetime)
test['query_datetime'] = pd.to_datetime(test.query_datetime)


#-------------------------
# train
#-------------------------
#每一個程式開始的時間
df1 = train.groupby('id').query_datetime.min().to_frame().reset_index()
df1.rename(columns = {'query_datetime': 'initial_time'}, inplace = True)
#每一個程式開始的結束的時間 
df2 = train.groupby('id').query_datetime.max().to_frame().reset_index()
df2.rename(columns = {'query_datetime': 'final_time'}, inplace = True)
#每一個程式發生可以的紀錄總共有多少次數
df3 = train.groupby('id').query_datetime.count().to_frame('total_count').reset_index()
#每一個程式的持續時間
train = pd.merge(df1, df2, on = 'id', how = 'left')
train = pd.merge(train, df3, on = 'id', how = 'left')
train['duration'] = (train.final_time - train.initial_time).map(lambda x: x.days)
train['duration_in_sec'] = (train.final_time - train.initial_time).map(lambda x: x.total_seconds())
#是否持續時間沒有超過一天的
train['duration_is_0'] = [1 if i == 0 else 0 for i in train.duration]
#該程式發生頻繁的程度 over duration
train['freq_over_time'] = 1.0 * train['total_count'] / (train['duration'] + 1) # smoothing
##some features with largest, we perform log transformation
train['log_freq'] = np.log(train.freq_over_time)



#-------------------------
# test
#-------------------------

#每一個程式開始的時間
df1 = test.groupby('id').query_datetime.min().to_frame().reset_index()
df1.rename(columns = {'query_datetime': 'initial_time'}, inplace = True)
#每一個程式開始的結束的時間 
df2 = test.groupby('id').query_datetime.max().to_frame().reset_index()
df2.rename(columns = {'query_datetime': 'final_time'}, inplace = True)
#每一個程式發生可以的紀錄有多少次數
df3 = test.groupby('id').query_datetime.count().to_frame('total_count').reset_index()
#每一個程式的持續時間
test = pd.merge(df1, df2, on = 'id', how = 'left')
test = pd.merge(test, df3, on = 'id', how = 'left')
test['duration'] = (test.final_time - test.initial_time).map(lambda x: x.days)
test['duration_in_sec'] = (test.final_time - test.initial_time).map(lambda x: x.total_seconds())
#是否持續時間沒有超過一天的
test['duration_is_0'] = [1 if i == 0 else 0 for i in test.duration]
#該程式發生頻繁的程度 over duration
test['freq_over_time'] = 1.0 * test['total_count'] / (test['duration'] + 1) # smoothing
##some features with largest, we perform log transformation
test['log_freq'] = np.log(test.freq_over_time)
#-------------------------
#save
#-------------------------

train.to_csv('../feature/{}/duration.csv.gz'.format('train'), index = False, compression='gzip')
test.to_csv('../feature/{}/duration.csv.gz'.format('test'), index = False, compression='gzip')







